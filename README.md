# Mooc_Dropout_Prediction

## Description
Students' high dropout rate on MOOC platforms (e.g., Coursera) has been heavily criticized, and predicting their likelihood of dropout would be useful for maintaining and encouraging students' learning activities. In this competition, you are challenged to build a predictor that can predict the chance that a student will drop out of an enrollment after observing his/her early course activities. In particular, you have access to the student's course-relevant activities, such as working on course assignments, watching course videos, accessing the course wiki, etc.

## Data preparation
We can download four csv file from Kaggle.
(1)	enrollment_list.csv - each line is a course enrollment record with an enrollment_id E, a user_id U and a course_id C, indicating that U has enrolled in C. After looking through this file carefully, we find that some students take one or more courses at a time; some courses are popular. So we can count the courses each students take and the number of students in each courses. By combining the dropout label, we can count how many courses this student drops out. If this student has already dropped out many courses, it means he has a large probability to drop out other courses, so we finally calculate the probability of dropping out for each student. Besides, some courses may hard and it’s more likely for students to drop out. So, we also calculate the probability of dropping out for each course. We extract two features from this file.

(2)	activity_log.csv - each line is a behavior record called "event". Each event contains the following information: enrollment_id, time, and event. This file contains lots of information. So, we need to find the features carefully. First, it’s obviously that if this student has lots of event logs, it means he is really diligent, so the number of logs can be a reference. However, after we sort them in terms of the number of logs. It shows even some students only have one log, he doesn’t drop out actually. Some students have many logs, but they still drop out. So, the number of logs can only be a reference rather than the most important features. Second, the time can also be a feature. Some students are a ‘3-minute passion’, they may have lots of logs, but they can only hold it for only one day. If a student insists and have logs every day, that means he really work hard for his courses. So, we extract the number of days for each student. Third, students have different types of events like problem, video, wiki. Some of them are important and others may not, but we don’t know which is important. we must count all of them and put it into the classifier which can decide which is important.

(3)	train_label.csv - each line is a dropout record of the enrollments in the training set. Each record contains two fields, enrollment_id E and dropout_prob, with dropout_prob = 1 indicating the student drops out of E. After we extract so many features, we must put them together. So, I put them into a sum_data.csv (figure 1) file to make them more easier process. 

(4)	sample_submission.csv - each line is a dropout record of the enrollments in the test set, which you are required to predict.

## Models and results
There are lots of classification models in scikit-learn, First, I try the SVM, Support vector machines (SVMs) are a set of supervised learning methods used for classification, regression and outlier’s detection. I use the default parameters at first, the final results are 0.37, the training time is about 45 minutes and it’s really long compared to the other models. After reading the document, I know SVMs do not directly provide probability estimates, these are calculated using an expensive five-fold cross-validation.

The second model I tried Neural network models. Multi-layer Perceptron (MLP) is a supervised learning algorithm that learns a function   by training on a dataset, where   is the number of dimensions for input and   is the number of dimensions for output. Given a set of features   and a target  , it can learn a non-linear function approximator for either classification or regression. It is different from logistic regression, in that between the input and the output layer, there can be one or more non-linear layers, called hidden layers. Figure shows a one hidden layer MLP with scalar output.
 
The result become 0.35 after I changed the model from SVM to MLP by default parameters.
There are several key parameters that need to be introduced. The first is the hidden layer size. It’s determines how complicated of this model. With small number of layers, it can’t classify exactly. With large number of layers. It may be faced with overfitting problem. So, it need a fine tuning. I tried some sizes: 300,500,800,1000,1500,2000. And the best result is come from 800 and 1000.
The second is activation which means the activation function for the hidden layer. The default ReLU is the best after testing.
Solver is also an important parameter. The default solver ‘adam’ works pretty well on relatively large datasets (with thousands of training samples or more) in terms of both training time and validation score. For small datasets, however, ‘lbfgs’ can converge faster and perform better. So I choose adam as the solver. ‘sgd’ is also a good choice, but adam can completely replace it.
Batch_size means the size of minibatches for stochastic optimizers. If it’s too small, the gradient will get down slowly. If it’s large, the training time will become long. The default is 200 which seems too small for the number of the training set. So, I set the parameters to 800.
Learning rate ‘adaptive’ keeps the learning rate constant to ‘learning_rate_init’ as long as training loss keeps decreasing. Each time two consecutive epochs fail to decrease training loss by at least tol, or fail to increase validation score by at least tol if ‘early_stopping’ is on, the current learning rate is divided by 5. This is much better comparing to the default constant.
Early_stopping is a good method to avoid overfitting. Whether to use early stopping to terminate training when validation score is not improving. If set to true, it will automatically set aside 10% of training data as validation and terminate training when validation score is not improving by at least tol for two consecutive epochs.
Actually, the first result is not good after I use MLP because the features I extract is not same with the type, some are numbers, some are probability and some number is really large. So, normalization is a good way to solve this problem, I norm all of these numbers to 0~1, and multiply them with 100, so they can be mapped to 0~100. These numbers exactly work well on this model.
There are also a tricky way to improve your final results. Some samples are hard to predict, you have no idea of what exactly whether this guy will drop out or not. According to the evaluation, if the probability is too small, the final answer will be influenced seriously if it predicts a wrong result. For example, if you predict that all the probability is 1, the final result is 6.0. If you predict that all the probability is 0.5, the final result is 0.6 which is much better than 6.0. So avoiding a extreme value will help you to improve your final result. I get a 0.07 down after I use this way.

## What I learned from doing the project
(1)	How to read the csv file and extract the features from it. At, first I just write all the code together and I find it’s a little time consuming because when you change the parameters of the model, you do not need to process the file again. Then, I separate them and save my lots of time and make code more readable and concise.

(2)	 Learning the different model and try to compare them. I use SVM at first, the result is not good and it takes a long time to train. It also leaves you a pretty small space for you to adjust this, so I change it to MLP which performs really good, I also try other models, but only two of them are best, they can work well on such big dataset and save time. It took me 10 seconds to train the model and got the final results.

(3)	How to adjust the model. You must change one parameter at a time and keep other variables stable.

(4)	How to extract features from dataset. By using our daily experience, we can think what features are important and influence the final results. You may not no which feature is most important, but you still need to extract them.

(5)	How to debug. Frist, I use the number of the student of one class, the number is really large and got a bad result. When I delete this feature, the result gets better. So, I think this feature may have some problem applying to this model. After reading some relative materials, I use normalization to solve it.

